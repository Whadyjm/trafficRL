# Sistema de Control de Tr치fico H칤brido con RL (PPO)

Este proyecto implementa un sistema de control de sem치foros inteligente utilizando Aprendizaje por Refuerzo (Reinforcement Learning - RL) con el algoritmo PPO (Proximal Policy Optimization). El sistema est치 dise침ado para operar en un entorno de simulaci칩n SUMO, priorizando veh칤culos de emergencia (ambulancias) y peatones, mientras optimiza el flujo vehicular general.

## 游 L칩gica del Sistema (Hybrid Control)

El "cerebro" del sistema no es solo una red neuronal; es un controlador h칤brido que toma decisiones basadas en una jerarqu칤a de prioridades estricta implementada en la clase `EntornoOptimizado`:

### 1. Prioridad M치xima: Emergencia (Ambulancia) 游뚬
*   **Detecci칩n**: El sistema escanea constantemente la red buscando veh칤culos de tipo "ambulancia".
*   **L칩gica**: Si una ambulancia es detectada en un carril controlado por el sem치foro:
    1.  Identifica qu칠 fase verde permite el paso a ese carril espec칤fico.
    2.  **Override**: Ignora cualquier decisi칩n del modelo RL o temporizador.
    3.  **Acci칩n Inmediata**: Fuerza el cambio a la fase verde de la ambulancia, manipulando internamente los contadores de `min_green` para evitar retrasos de seguridad est치ndar del sem치foro.
*   **Objetivo**: Tiempo de espera cero para emergencias.

### 2. Prioridad Secundaria: Horario Peatonal Programado 游뛌
*   **Condici칩n**: Si **no** hay ambulancia presente Y **hay peatones activos** (esperando o cruzando).
*   **L칩gica**: Se basa en el tiempo de ciclo de la simulaci칩n (137 segundos en total).
    *   **Ventana 1 (Segundos 23-38)**: Se fuerza la **Fase Peatonal 1** SOLO si se detecta actividad peatonal.
    *   **Ventana 2 (Segundos 122-137)**: Se fuerza la **Fase Peatonal 2** SOLO si se detecta actividad peatonal.
*   **Objetivo**: Garantizar ventanas de cruce seguras para peatones cuando son necesarias, evitando detener el tr치fico vehicular innecesariamente si no hay nadie esperando.

### 3. Prioridad Terciaria: Agente Inteligente (RL - PPO) 游뱄
*   **Condici칩n**: Si no hay emergencias ni es horario peatonal reservado.
*   **L칩gica**: El modelo PPO toma el control total.
*   **Input (Observaci칩n)**: Recibe un vector que incluye:
    *   Fase actual (One-hot encoding).
    *   Tiempo m칤nimo de verde cumplido (Binario).
    *   Densidad y cantidad de veh칤culos en cola por carril.
    *   Flags de presencia de ambulancia (para que aprenda a anticipar, aunque la regla 1 fuerce la acci칩n).
*   **Output (Acci칩n)**: Selecciona la siguiente fase verde 칩ptima para minimizar la funci칩n de coste (Reward).

---

## 游늭 Archivos Principales

### 1. `entrenamiento.py`
Script encargado de entrenar el modelo. Define la "Funci칩n de Recompensa" que gu칤a el aprendizaje:

*   **Recompensas (+) y Castigos (-)**:
    *   **Ambulancia**: `-1000 * (tiempo_espera^2)` (Castigo extremo si espera) | `+500` (Premio si fluye).
    *   **Peatones**: `-400 * (espera^2)` (Evitar aglomeraciones) | `+150` por cada peat칩n que cruza.
    *   **Veh칤culos**: `-5 * (espera^2)` (Fluidez general) | `-50` por "Tiempos Muertos" (sem치foro en verde sin autos pasando).
    *   **Seguridad**: `-2000` por colisiones | `-100` por infracciones (cruzar en rojo/jaywalking).

### 2. `despliegue.py`
Script para probar y visualizar el modelo entrenado en tiempo real.
*   **Consistencia**: Importa y redefine la misma clase `EntornoOptimizado` y `AmbulanceObservationFunction` que el entrenamiento. Esto es crucial para que el modelo cargado interprete el estado de la simulaci칩n exactamente igual que como fue entrenado.
*   **Visualizaci칩n**: Ejecuta SUMO con interfaz gr치fica (`use_gui=True`) y muestra m칠tricas en consola.

---

## 游 C칩mo Ejecutar

### Requisitos Previos
*   Python 3.x
*   SUMO (Simulation of Urban MObility) instalado y en el PATH.
*   Librer칤as Python: `sumo-rl`, `stable-baselines3`, `gymnasium`, `traci`, `pandas`, `matplotlib`.

### 1. Entrenamiento
Ejecuta el script para iniciar el proceso de aprendizaje. Esto crear치 el archivo del modelo `.zip` y guardar치 logs de progreso.
```bash
python entrenamiento.py
```
*Salida*: `trigal_model_ambulancia.zip` y carpeta `outputs_optimizados/`.

### 2. Despliegue (Inferencia)
Una vez entrenado (o si ya tienes el `.zip`), ejecuta el despliegue para ver al agente en acci칩n.
```bash
python despliegue.py
```
*Nota*: Si el modelo no existe, el script fallar치. Aseg칰rate de haber entrenado primero.
